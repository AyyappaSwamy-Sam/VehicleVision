{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNsHJxVARVmr"
      },
      "source": [
        "# INTRODUCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eyWwo1YRsao"
      },
      "source": [
        "This project illustrates the detection and tracking of vehicles using Kalman Filter. Following operations are performed in this analysis:\n",
        "\n",
        "1. Prediction of current and future location of the vehicle.\n",
        "2. Correcting the prediction as per the new measurements attained.\n",
        "3. Optimizing the noise created by faulty detections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2k2ZX4jDium"
      },
      "source": [
        "In this analysis, we detect and track multiple vehicles using a camera mounted inside a self\u0002driving car. The images captured by the camera are taken as the input. In the image, the car is captured and a bounding box is created around the car. The model “ssd_mobilenet_v1_coco” which is pretrained on MS COCO datset is used for the detection of car. \n",
        "\n",
        "The coordinates of all four corners of the bounding are noted and state of the vehicle is determined at a particular time. Here, the rate of change of position gives the velocity. We consider the velocity as constant in the analysis. (Hence, acceleration is zero.)\n",
        "\n",
        "Then, we implement Kalman filter to perform two main operations:\n",
        "\n",
        "1. Prediction of objects current location using the previous states.\n",
        "2. In update, current measurements are used to correct the state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ObMsfZSXVJ"
      },
      "source": [
        "# STATE EQUATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAY5al_EF5X2"
      },
      "source": [
        "The general linear dynamic system's state equation used is of the form: \n",
        "$x_k = x_{k-1} + u_{k-1} \\Delta t + 0.5 * a_{k-1} * \\Delta t^2$\n",
        "\n",
        "\n",
        "\n",
        "where,\n",
        "\n",
        "$x_k$ = state at a given time stamp 'k'  (current state)\n",
        "\n",
        "$x_{k-1}$ = = state at a given time 'k-1' (prior state)\n",
        "\n",
        "$\\Delta t $ is change in time\n",
        "\n",
        "u is the velocity at time 'k-1'   (which is controlling the satates)\n",
        "\n",
        "a is acceleration at time 'k-1'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v00WRRxPI3wP"
      },
      "source": [
        "In this analysis, we consider the object travelling in 2-D (both in X and Y direction).\n",
        "\n",
        "Hence there will be state equations in terms of x and y.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECQvS6pDJOLT"
      },
      "source": [
        "Here, we are considering the four corners of the bounding box. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSNqoNXtKVfK"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cesBx0iYKXZF"
      },
      "source": [
        "In a X-Y plane: \n",
        "\n",
        "y1 = y2 ; y4 = y3 (since the pair of points are on same horizontal line)\n",
        "\n",
        "x1 = x4 ; x2 = x3 (since the pair of points are on same verticle line)\n",
        "\n",
        "Hence, there will be four state equtions:\n",
        "\n",
        "$x_{1, k} = x_{1, k-1} + u_{1,k-1} \\Delta t $ \n",
        "\n",
        "$x_{2, k} = x_{2, k-1} + u_{2,k-1} \\Delta t $\n",
        "\n",
        "$y_{1, k} = y_{1, k-1} + u_{1,k-1} \\Delta t $\n",
        "\n",
        "$y_{1, k} = y_{1, k-1} + u_{1,k-1} \\Delta t $\n",
        "\n",
        "(As already mentioned acceletaion term is zero, since velocity os considered constant.)\n",
        "\n",
        "Where u is velocity at k-1 timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUGa__TGMZLU"
      },
      "source": [
        "The state equation are always paired wih measurement equations, that describes the relationship between state and measurement at the current time stamp k.\n",
        "\n",
        "The corresponding equations are obtained by taking the first derivative of staes with respect to time:\n",
        "\n",
        "$ z_{x1, k} = u_{x1,k-1} $\n",
        "\n",
        "$ z_{x2, k} = u_{x2,k-1} $\n",
        "\n",
        "$ z_{y1, k} = u_{y1,k-1} $\n",
        "\n",
        "$ z_{y2, k} = u_{y2,k-1} $\n",
        "\n",
        "\n",
        "Where u is velocity at k-1 timestamp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoeQ6fwAVf7a"
      },
      "source": [
        "### Notations:\n",
        "\n",
        "X - State Mean\n",
        "\n",
        "P - State Covariance\n",
        "\n",
        "F - State Transition Matrix\n",
        "\n",
        "Q - Process Covariance\n",
        "\n",
        "B - Control Function\n",
        "\n",
        "u - Control Input\n",
        "\n",
        "\n",
        "Here, Q consists of the variances associated with each of the state estimates as well as the correlation between the errors in the state estimates.\n",
        "$Q = \\begin{bmatrix} \n",
        "  \\Delta t^4 / 4   &\\Delta t^3 / 2 \\\\\n",
        "  \\Delta t^3 / 2   &\\Delta t^2  \\\\\n",
        "  \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsqglWIVPQrm"
      },
      "source": [
        "All the variables and their respective coeffecients are grouped in the form of a matrices:\n",
        "\n",
        "1. State Transition matrix: $F = \\begin{bmatrix} \n",
        "  1 &\\Delta t &0 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &1 &0 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &1 &\\Delta t &0 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &1 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &1 &\\Delta t &0 &0 \\\\\n",
        "  0 &0 &0 &0 &0 &1 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &0 &0 &1 &\\Delta t \\\\\n",
        "  0 &0 &0 &0 &0 &0 &0 &1\n",
        "  \\end{bmatrix}$\n",
        "  This contains all the coffecients of state equation.\n",
        "\n",
        "\n",
        "2. Measurement matrix: $B = \\begin{bmatrix} \n",
        "  1 &0 &0 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &1 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &1 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &0 &0 &1 &0 \\\\\n",
        "  \\end{bmatrix}$\n",
        "  This is also known as Control matrix, which contains all the coeffecients of measurement equation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6f5vAYQSZ-N"
      },
      "source": [
        "\n",
        "### ***Prediction phase equations***:\n",
        "\n",
        "$\\bar{x} = Fx + Bu$, where \n",
        "\n",
        "u = control input matrix\n",
        "\n",
        "$\\bar{P} = FPF^T + Q$\n",
        "\n",
        "\n",
        "Here, \n",
        "\n",
        " $Fx = \\begin{bmatrix}\n",
        "  y_{1,k-1} + u_{y1,k-1} \\Delta t \\\\\n",
        "  u_{y1,k-1} \\\\\n",
        "  x_{1 k-1} + u_{x1,k-1} \\Delta t \\\\\n",
        "  u_{x1,k-1} \\\\\n",
        "  y_{2,k-1} + u_{y2,k-1} \\Delta t \\\\\n",
        "  u_{y2,k-1} \\\\\n",
        "  x_{2,k-1} + u_{x2,k-1} \\Delta t \\\\\n",
        "  u_{x2,k-1}\n",
        "  \\end{bmatrix}$,\n",
        "\n",
        "  \n",
        "  \n",
        "  $ Bu = \\begin{bmatrix} y_{1 k-1} \\\\ x_{1 k-1} \\\\ y_{2 k-1} \\\\x_{2 k-1}\\end{bmatrix}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6scRv8YWol5"
      },
      "source": [
        "### ***Update phase equations***:\n",
        "\n",
        "H - Measurement matrix\n",
        "\n",
        "z - measurement\n",
        "\n",
        "R - measiurement noise covariance\n",
        "\n",
        "y - reisdual\n",
        "\n",
        "K - Klaman gain   \n",
        "\n",
        "\n",
        "Here, \n",
        "\n",
        "H is a transformation matrix that transforms the state into the measurement domain\n",
        "\n",
        "Klaman gain is just a weight given to the measurements and the current state estimates)\n",
        "\n",
        "Residual is difference between the measurement and the value predicted by the filter.\n",
        "\n",
        "\n",
        "\n",
        "$y = z - H\\bar{x}$\n",
        "\n",
        "$K =  \\bar{P}H^T(H\\bar{P}H^T + R)^-1 $ \n",
        "\n",
        "$x =  \\bar{x} + Ky $ \n",
        "\n",
        "$P = (I - KH)\\bar{P}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPxqGdzKVB1b"
      },
      "source": [
        "\n",
        "# INSTALLING THE REQUIRED PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OERBr-xAQQry"
      },
      "outputs": [],
      "source": [
        "#installing packages\n",
        "!pip install fastai\n",
        "!pip install -Uqq fastbook\n",
        "!pip install matplotlib-venn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDod3u7wUIK8"
      },
      "source": [
        "# Cloning Github link to google colab File folder:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvX-ZlFCfiE"
      },
      "source": [
        "# Implementation and test car detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Hp5HlQCa74"
      },
      "source": [
        "In this phase of the analysis, the images/frames are taken as input. By using the model 'ssd_mobilenet_v1_coco' model, car is detected in the image and bounding boxes are formed, which is output. The coordinate axes of four corners of the box are considered and states are defined in futher analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haAE75TBk5b5",
        "outputId": "d56c0f3f-5466-4679-a0cb-ffb031448de3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "cwd = os.path.dirname(os.path.realpath('E:\\\\maths project\\\\VehicleVision\\\\test_images'))\n",
        "\n",
        "class CarDetector(object):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.car_boxes = []\n",
        "        \n",
        "        os.chdir(cwd)\n",
        "        \n",
        "        # Tensorflow localization/detection model\n",
        "        # Single-shot-dectection with mobile net architecture trained on COCO dataset\n",
        "        \n",
        "        detect_model_name = 'E:\\\\maths project\\\\VehicleVision\\\\Pre Trained Model'\n",
        "\n",
        "        PATH_TO_CKPT = detect_model_name + '\\\\frozen_inference_graph.pb'\n",
        "        \n",
        "        # setup tensorflow graph\n",
        "        self.detection_graph = tf.Graph()\n",
        "        \n",
        "        # configuration for possible GPU use\n",
        "        config = tf.compat.v1.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        # load frozen tensorflow detection model and initialize \n",
        "        # the tensorflow graph\n",
        "        with self.detection_graph.as_default():\n",
        "          od_graph_def = tf.compat.v1.GraphDef()\n",
        "          with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "            serialized_graph = fid.read()\n",
        "            od_graph_def.ParseFromString(serialized_graph)\n",
        "            tf.import_graph_def(od_graph_def, name='')\n",
        "               \n",
        "            self.sess = tf.compat.v1.Session(graph=self.detection_graph, config=config)\n",
        "            self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "              # Each box represents a part of the image where a particular object was detected.\n",
        "            self.boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "              # Each score represent how level of confidence for each of the objects.\n",
        "              # Score is shown on the result image, together with the class label.\n",
        "            self.scores =self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            self.classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            self.num_detections =self.detection_graph.get_tensor_by_name('num_detections:0')\n",
        "    \n",
        "    # Helper function to convert image into numpy array    \n",
        "    def load_image_into_numpy_array(self, image):\n",
        "         (im_width, im_height) = image.size\n",
        "         return np.array(image.getdata()).reshape(\n",
        "            (im_height, im_width, 3)).astype(np.uint8)       \n",
        "    # Helper function to convert normalized box coordinates to pixels\n",
        "    def box_normal_to_pixel(self, box, dim):\n",
        "    \n",
        "        height, width = dim[0], dim[1]\n",
        "        box_pixel = [int(box[0]*height), int(box[1]*width), int(box[2]*height), int(box[3]*width)]\n",
        "        return np.array(box_pixel)       \n",
        "        \n",
        "    def get_localization(self, image, visual=False):  \n",
        "        \n",
        "        \"\"\"Determines the locations of the cars in the image\n",
        "\n",
        "        Args:\n",
        "            image: camera image\n",
        "\n",
        "        Returns:\n",
        "            list of bounding boxes: coordinates [y_up, x_left, y_down, x_right]\n",
        "\n",
        "        \"\"\"\n",
        "        category_index={1: {'id': 1, 'name': u'person'},\n",
        "                        2: {'id': 2, 'name': u'bicycle'},\n",
        "                        3: {'id': 3, 'name': u'car'},\n",
        "                        4: {'id': 4, 'name': u'motorcycle'},\n",
        "                        5: {'id': 5, 'name': u'airplane'},\n",
        "                        6: {'id': 6, 'name': u'bus'},\n",
        "                        7: {'id': 7, 'name': u'train'},\n",
        "                        8: {'id': 8, 'name': u'truck'},\n",
        "                        9: {'id': 9, 'name': u'boat'},\n",
        "                        10: {'id': 10, 'name': u'traffic light'},\n",
        "                        11: {'id': 11, 'name': u'fire hydrant'},\n",
        "                        13: {'id': 13, 'name': u'stop sign'},\n",
        "                        14: {'id': 14, 'name': u'parking meter'}}  \n",
        "        \n",
        "        # The following code snippet implements the actual detection using TensorFlow API.\n",
        "\n",
        "        with self.detection_graph.as_default():\n",
        "              image_expanded = np.expand_dims(image, axis=0)\n",
        "              (boxes, scores, classes, num_detections) = self.sess.run(\n",
        "                  [self.boxes, self.scores, self.classes, self.num_detections],\n",
        "                  feed_dict={self.image_tensor: image_expanded})\n",
        "\n",
        "        # Here boxes, scores, and classes represent the bounding box, confidence level, and class name corresponding to each of the detection, respectively. \n",
        "        # In the next step, we select the detections that are cars and have a confidence greater than a threshold ( e.g., 0.3 in this case).\n",
        "\n",
        "\n",
        "              if visual == True:\n",
        "                  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "                      image,\n",
        "                      np.squeeze(boxes),\n",
        "                      np.squeeze(classes).astype(np.int32),\n",
        "                      np.squeeze(scores),\n",
        "                      category_index,\n",
        "                      use_normalized_coordinates=True,min_score_thresh=.4,\n",
        "                      line_thickness=3)\n",
        "    \n",
        "                  plt.figure(figsize=(9,6))\n",
        "                  plt.imshow(image)\n",
        "                  plt.show()  \n",
        "              \n",
        "              boxes=np.squeeze(boxes)\n",
        "              classes =np.squeeze(classes)\n",
        "              scores = np.squeeze(scores)\n",
        "    \n",
        "              cls = classes.tolist()\n",
        "              \n",
        "              # The ID for car in COCO data set is 3 \n",
        "              idx_vec = [i for i, v in enumerate(cls) if ((v==3) and (scores[i]>0.3))]\n",
        "              \n",
        "              if len(idx_vec) ==0:\n",
        "                  print('no detection!')\n",
        "                  self.car_boxes = []  \n",
        "              else:\n",
        "                  tmp_car_boxes=[]\n",
        "                  for idx in idx_vec:\n",
        "                      dim = image.shape[0:2]\n",
        "                      box = self.box_normal_to_pixel(boxes[idx], dim)\n",
        "                      box_h = box[2] - box[0]\n",
        "                      box_w = box[3] - box[1]\n",
        "                      ratio = box_h/(box_w + 0.01)\n",
        "\n",
        "\n",
        "                      #To further reduce possible false positives, we include thresholds for bounding box width, height, and height-to-width ratio.\n",
        "                      \n",
        "                      if ((ratio < 0.8) and (box_h>20) and (box_w>20)):\n",
        "                          tmp_car_boxes.append(box)\n",
        "                          print(box, ', confidence: ', scores[idx], 'ratio:', ratio)\n",
        "                         \n",
        "                      else:\n",
        "                          print('wrong ratio or wrong size, ', box, ', confidence: ', scores[idx], 'ratio:', ratio)\n",
        "                          \n",
        "                          \n",
        "                  \n",
        "                  self.car_boxes = tmp_car_boxes\n",
        "             \n",
        "        return self.car_boxes\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "        # Test the performance of the detector\n",
        "        det =CarDetector()\n",
        "        os.chdir(cwd)\n",
        "        TEST_IMAGE_PATHS= glob(os.path.join('E:\\\\maths project\\\\VehicleVision\\\\test_images\\\\', '*.jpg'))\n",
        "        \n",
        "        for i, image_path in enumerate(TEST_IMAGE_PATHS[0:2]):\n",
        "            print('')\n",
        "            print('*************************************************')\n",
        "            \n",
        "            img_full = Image.open(image_path)\n",
        "            img_full_np = det.load_image_into_numpy_array(img_full)\n",
        "            img_full_np_copy = np.copy(img_full_np)\n",
        "            start = time.time()\n",
        "            b = det.get_localization(img_full_np, visual=False)\n",
        "            end = time.time()\n",
        "            print('Localization time: ', end-start)\n",
        "#            \n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdRw8XSV2TpO"
      },
      "source": [
        "# Helper classes and functions for detection and tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q37owMD2fIs"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "class Box:\n",
        "    def __init__(self):\n",
        "        self.x, self.y = float(), float()\n",
        "        self.w, self.h = float(), float()\n",
        "        self.c = float()\n",
        "        self.prob = float()\n",
        "\n",
        "def overlap(x1,w1,x2,w2):\n",
        "    l1 = x1 - w1 / 2.;\n",
        "    l2 = x2 - w2 / 2.;\n",
        "    left = max(l1, l2)\n",
        "    r1 = x1 + w1 / 2.;\n",
        "    r2 = x2 + w2 / 2.;\n",
        "    right = min(r1, r2)\n",
        "    return right - left;\n",
        "\n",
        "def box_intersection(a, b):\n",
        "    w = overlap(a.x, a.w, b.x, b.w);\n",
        "    h = overlap(a.y, a.h, b.y, b.h);\n",
        "    if w < 0 or h < 0: return 0;\n",
        "    area = w * h;\n",
        "    return area;\n",
        "\n",
        "def box_union(a, b):\n",
        "    i = box_intersection(a, b);\n",
        "    u = a.w * a.h + b.w * b.h - i;\n",
        "    return u;\n",
        "\n",
        "def box_iou(a, b):\n",
        "    return box_intersection(a, b) / box_union(a, b);\n",
        "\n",
        "def box_iou2(a, b):\n",
        "    '''\n",
        "    Helper funciton to calculate the ratio between intersection and the union of\n",
        "    two boxes a and b\n",
        "    a[0], a[1], a[2], a[3] <-> left, up, right, bottom\n",
        "    '''\n",
        "    \n",
        "    w_intsec = np.maximum (0, (np.minimum(a[2], b[2]) - np.maximum(a[0], b[0])))\n",
        "    h_intsec = np.maximum (0, (np.minimum(a[3], b[3]) - np.maximum(a[1], b[1])))\n",
        "    s_intsec = w_intsec * h_intsec\n",
        "    s_a = (a[2] - a[0])*(a[3] - a[1])\n",
        "    s_b = (b[2] - b[0])*(b[3] - b[1])\n",
        "  \n",
        "    return float(s_intsec)/(s_a + s_b -s_intsec)\n",
        "\n",
        "def convert_to_pixel(box_yolo, img, crop_range):\n",
        "    \n",
        "    box = box_yolo\n",
        "    imgcv = img\n",
        "    [xmin, xmax] = crop_range[0]\n",
        "    [ymin, ymax] = crop_range[1]\n",
        "    h, w, _ = imgcv.shape\n",
        "    \n",
        "    # Calculate left, top, width, and height of the bounding box\n",
        "    left = int((box.x - box.w/2.)*(xmax - xmin) + xmin)\n",
        "    top = int((box.y - box.h/2.)*(ymax - ymin) + ymin)\n",
        "    \n",
        "    width = int(box.w*(xmax - xmin))\n",
        "    height = int(box.h*(ymax - ymin))\n",
        "    \n",
        "    # Deal with corner cases\n",
        "    if left  < 0    :  left = 0\n",
        "    if top   < 0    :   top = 0\n",
        "    \n",
        "    # Return the coordinates (in the unit of the pixels)\n",
        "  \n",
        "    box_pixel = np.array([left, top, width, height])\n",
        "    return box_pixel\n",
        "\n",
        "\n",
        "\n",
        "def convert_to_cv2bbox(bbox, img_dim = (1280, 720)):\n",
        "    '''\n",
        "    Helper fucntion for converting bbox to bbox_cv2\n",
        "    bbox = [left, top, width, height]\n",
        "    bbox_cv2 = [left, top, right, bottom]\n",
        "    img_dim: dimension of the image, img_dim[0]<-> x\n",
        "    img_dim[1]<-> y\n",
        "    '''\n",
        "    left = np.maximum(0, bbox[0])\n",
        "    top = np.maximum(0, bbox[1])\n",
        "    right = np.minimum(img_dim[0], bbox[0] + bbox[2])\n",
        "    bottom = np.minimum(img_dim[1], bbox[1] + bbox[3])\n",
        "    \n",
        "    return (left, top, right, bottom)\n",
        "    \n",
        "    \n",
        "def draw_box_label(img, bbox_cv2, box_color=(0, 255, 255), show_label=True):\n",
        "    '''\n",
        "    Helper funciton for drawing the bounding boxes and the labels\n",
        "    bbox_cv2 = [left, top, right, bottom]\n",
        "    '''\n",
        "    #box_color= (0, 255, 255)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_size = 0.7\n",
        "    font_color = (0, 0, 0)\n",
        "    left, top, right, bottom = bbox_cv2[1], bbox_cv2[0], bbox_cv2[3], bbox_cv2[2]\n",
        "    \n",
        "    # Draw the bounding box\n",
        "    cv2.rectangle(img, (left, top), (right, bottom), box_color, 4)\n",
        "    \n",
        "    if show_label:\n",
        "        # Draw a filled box on top of the bounding box (as the background for the labels)\n",
        "        cv2.rectangle(img, (left-2, top-45), (right+2, top), box_color, -1, 1)\n",
        "        \n",
        "        # Output the labels that show the x and y coordinates of the bounding box center.\n",
        "        text_x= 'x='+str((left+right)/2)\n",
        "        cv2.putText(img,text_x,(left,top-25), font, font_size, font_color, 1, cv2.LINE_AA)\n",
        "        text_y= 'y='+str((top+bottom)/2)\n",
        "        cv2.putText(img,text_y,(left,top-5), font, font_size, font_color, 1, cv2.LINE_AA)\n",
        "    \n",
        "    return img    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoFPh2ayDLJr"
      },
      "source": [
        "# Implementing and test tracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7HZ-CIIDOI9"
      },
      "source": [
        "The operations - prediction and update take place in this phase. \n",
        "\n",
        "In prediction, the  thye previous statres are used to predict the current state. \n",
        "In update, the current measurement value (location of the bounding box) is used, to correct the state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1J-7WhfDcE2"
      },
      "source": [
        "Here, we use the coordinates and their first-order derivatives of the up, down, left and right cornerx of the bounding box.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "Here, we are assuming the constant velocity (thus no acceleration).\n",
        "\n",
        "Time interval is constant and is taken as 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "ShBeLsDx2rZ8",
        "outputId": "ca015075-23f2-49f2-81ca-f97520476372"
      },
      "outputs": [],
      "source": [
        "from numpy import dot\n",
        "from scipy.linalg import inv, block_diag\n",
        "import glob\n",
        "\n",
        "\n",
        "class Tracker():                                                                  # class for Kalman Filter-based tracker\n",
        "    def __init__(self):\n",
        "        # Initializing the parametes for tracker (history)\n",
        "        self.id = 0                                                               # tracker's id \n",
        "        self.box = []                                                             # list to store the coordinates for a bounding box \n",
        "        self.hits = 0                                                             # number of detection matches\n",
        "        self.no_losses = 0                                                        # number of unmatched tracks (track loss)\n",
        "        \n",
        "        # Initializing the parameters:\n",
        "        self.x_state=[] \n",
        "        self.dt = 1.   # time interval\n",
        "        \n",
        "        # Process matrix:\n",
        "        self.F = np.array([[1, self.dt, 0,  0,  0,  0,  0, 0],\n",
        "                           [0, 1,  0,  0,  0,  0,  0, 0],\n",
        "                           [0, 0,  1,  self.dt, 0,  0,  0, 0],\n",
        "                           [0, 0,  0,  1,  0,  0,  0, 0],\n",
        "                           [0, 0,  0,  0,  1,  self.dt, 0, 0],\n",
        "                           [0, 0,  0,  0,  0,  1,  0, 0],\n",
        "                           [0, 0,  0,  0,  0,  0,  1, self.dt],\n",
        "                           [0, 0,  0,  0,  0,  0,  0,  1]])\n",
        "        \n",
        "        \n",
        "        # Measurement matrix:\n",
        "        \n",
        "        self.H = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
        "                           [0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                           [0, 0, 0, 0, 1, 0, 0, 0], \n",
        "                           [0, 0, 0, 0, 0, 0, 1, 0]])\n",
        "        \n",
        "        \n",
        "        # Initialing the state covariance:\n",
        "        self.L = 10.0\n",
        "        self.P = np.diag(self.L*np.ones(8))\n",
        "        \n",
        "        \n",
        "        # Initializing the process covariance\n",
        "        self.Q_comp_mat = np.array([[self.dt**4/4., self.dt**3/2.],\n",
        "                                    [self.dt**3/2., self.dt**2]])\n",
        "        self.Q = block_diag(self.Q_comp_mat, self.Q_comp_mat, \n",
        "                            self.Q_comp_mat, self.Q_comp_mat)\n",
        "        \n",
        "        # Initializing the measurement covariance\n",
        "        self.R_scaler = 1.0\n",
        "        self.R_diag_array = self.R_scaler * np.array([self.L, self.L, self.L, self.L])\n",
        "        self.R = np.diag(self.R_diag_array)\n",
        "\n",
        "\n",
        "        # Here self.R_scaler represents the \"magnitude\" of measurement noise relative to state noise. \n",
        "        # A low self.R_scaler indicates a more reliable measurement. \n",
        "        \n",
        "        \n",
        "    def update_R(self):   \n",
        "        R_diag_array = self.R_scaler * np.array([self.L, self.L, self.L, self.L])\n",
        "        self.R = np.diag(R_diag_array)\n",
        "         \n",
        "        \n",
        "    def kalman_filter(self, z): \n",
        "        \n",
        "        # Implementing the Kalman Filter, including the predict and the update stages, with the measurement z\n",
        "        \n",
        "        x = self.x_state\n",
        "        \n",
        "        # Predict\n",
        "        x = dot(self.F, x)\n",
        "        self.P = dot(self.F, self.P).dot(self.F.T) + self.Q\n",
        "\n",
        "        #Update\n",
        "        S = dot(self.H, self.P).dot(self.H.T) + self.R\n",
        "        K = dot(self.P, self.H.T).dot(inv(S)) # Kalman gain\n",
        "        y = z - dot(self.H, x) # residual\n",
        "        x += dot(K, y)\n",
        "        self.P = self.P - dot(K, self.H).dot(self.P)\n",
        "        self.x_state = x.astype(int) # converting to integer coordinates \n",
        "                                     #(pixel values)\n",
        "        \n",
        "    def predict_only(self):  \n",
        "\n",
        "        # Implment only the predict stage. \n",
        "        # This is used for unmatched detections and unmatched tracks.\n",
        "\n",
        "        x = self.x_state\n",
        "        # Predict\n",
        "        x = dot(self.F, x)\n",
        "        self.P = dot(self.F, self.P).dot(self.F.T) + self.Q\n",
        "        self.x_state = x.astype(int)\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \n",
        "    # Creat an instance\n",
        "    trk = Tracker() \n",
        "    # Test R_ratio   \n",
        "    trk.R_scaler = 1.0/16\n",
        "    # Update measurement noise covariance matrix\n",
        "    trk.update_R()\n",
        "    # Initial state\n",
        "    x_init = np.array([390, 0, 1050, 0, 513, 0, 1278, 0])\n",
        "    x_init_box = [x_init[0], x_init[2], x_init[4], x_init[6]]\n",
        "    # Measurement\n",
        "    z=np.array([399, 1022, 504, 1256])\n",
        "    trk.x_state= x_init.T\n",
        "    trk.kalman_filter(z.T)\n",
        "    # Updated state\n",
        "    x_update =trk.x_state\n",
        "    x_updated_box = [x_update[0], x_update[2], x_update[4], x_update[6]]\n",
        "    \n",
        "    print('The initial state is: ', x_init)\n",
        "    print('The measurement is: ', z)\n",
        "    print('The update state is: ', x_update)\n",
        "    \n",
        "    # Visualize the Kalman filter process and the \n",
        "    # impact of measurement nosie convariance matrix\n",
        "\n",
        "    images = [plt.imread(file) for file in glob.glob('E:\\\\maths project\\\\VehicleVision\\\\test_images\\\\*.jpg')]\n",
        "    \n",
        "    img = images[1]\n",
        "    plt.figure(figsize=(10, 14))\n",
        "\n",
        "\n",
        "    # changed here\n",
        "    #helpers.draw_box_label(img, x_init_box, box_color=(0, 255, 0))\n",
        "    # draw_box_label(img, x_init_box, box_color=(0, 255, 0))\n",
        "    draw_box_label(img.copy(), x_init_box, box_color=(0, 255, 0))\n",
        "    ax = plt.subplot(3, 1, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Initial: '+str(x_init_box))\n",
        "\n",
        "    \n",
        "    # changed here\n",
        "    #helpers.draw_box_label(img, z, box_color=(255, 0, 0))\n",
        "    draw_box_label(img.copy(), z, box_color=(255, 0, 0))\n",
        "    ax = plt.subplot(3, 1, 2)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Measurement: '+str(z))\n",
        "    \n",
        "    #helpers.draw_box_label(img, x_updated_box)\n",
        "    draw_box_label(img.copy(), x_updated_box)\n",
        "    ax = plt.subplot(3, 1, 3)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Updated: '+str(x_updated_box))\n",
        "    plt.show()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naOjqYfvgr9-"
      },
      "source": [
        "# **OBSERVATION:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEN8JMpOETcF"
      },
      "source": [
        "From the above output images, we can visualize the measurement noises in the Kalman filter process. \n",
        "\n",
        "The green bounding box gives the initial state of the car. \n",
        "The red bounding box gives the measurement values.\n",
        "The aqua colored bounding box give sthe updated state. \n",
        "\n",
        "Here, the initial state value is 390\n",
        "The measurement state value is 399 afetr 1 second\n",
        "Updated state value is 398\n",
        "\n",
        "If measurement noise is low, the updated state is very close to the measurement. So, the aqua bounding box completely overlaps the red bounding box.\n",
        "\n",
        "Suppose, if measurement noise is high, the updated state is very close to the initial prediction. So, the aqua bounding box completely overlaps over the green bounding box.\n",
        "\n",
        "From the output images, there is no complete overlapping of green and aqua bounding box. Which infers that the measurement noise is relatively lower.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMp5xtjFFRYr"
      },
      "source": [
        "# ***MAIN functions***\n",
        "\n",
        "These functions, implementt the detection and tracking, including detection-track assignment and track management:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptAZHPsREyEI"
      },
      "source": [
        "In the below logic, the section assign_detections_to_trackers(trackers, detections, iou_thrd = 0.3) takes from current list of trackers and new detections, output matched detections, unmatched trackers, unmatched detections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrlvOWHdhohe"
      },
      "source": [
        "**HUNGARIAN ALGORITHM:**\n",
        "\n",
        "The Hungarian method is a combinatorial optimization algorithm that solves the assignment problem in polynomial time and which anticipated later primal–dual methods.\n",
        "(https://en.wikipedia.org/wiki/Hungarian_algorithm)\n",
        "\n",
        "\n",
        "EXAMPLE:\n",
        "\n",
        "Consider a company has 4 employees and there are four jobs to be done. Here, each employee is capable of doing any of the four jobs. But, the time taken to complete the work is different for different employees. Then Hungarian Algorithm concept is applied and the jobs are assigned to the emplyees in effecient manner so as to control the time and cost of the assignment.\n",
        "\n",
        "Here, the analysis is carried out by taking all the values of time taken by the employees for different jobs into a tablular format or in form of matrix (cost matrix) and operations are performed till we optimise the assignment.\n",
        "\n",
        "In the cost matrix, if a constant is added to every element of a row or a column, the optimum solution of the resulting assignment problem is the same as the original problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOADcePUFDsL"
      },
      "source": [
        "In this analysis, we implement Linear Assignment and Hungarian (Munkres) algorithm:\n",
        "\n",
        "\n",
        "If multiple detections are identified, we need to assign each of them to a tracker. Here, we are using intersection over union (IOU) of a 'tracker bounding box' and 'detection bounding box' as a metric. \n",
        "Here, we analyse till maximizing/optimizing the sum of IOU assignment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGFownwMFQ9E"
      },
      "source": [
        "In the below logic, linear_assignment by default minimizes an objective function. So we need to reverse the sign of IOU_mat for maximization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvrJw_IFXhj"
      },
      "source": [
        "**Unmatched detections and trackers:**\n",
        "\n",
        "\n",
        "Depending on the results of linear assignment, we make lists for unmatched detections and unmatched trackers. \n",
        "\n",
        "\n",
        "\n",
        "When a car enters into a frame and is first detected, it is not matched with any existing tracks. Hence this particular detection is categorized under unmatched detection. If any matching with an overlap less than iou_thrd, it denotes the existence of an untracked object. When the car leaves the frame, \n",
        "the previous used track has no more detections to consider. So, the track is considered as an unmatched track.\n",
        "\n",
        "In this way, the tracker and the detection associated in the matching are added to the lists of unmatched trackers and unmatched detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX3-KhM8GBcs"
      },
      "source": [
        "We include two important design parameters, min_hits and max_age, in the pipeline. The parameter min_hits is the number of consecutive matches needed to establish a track. The parameter max_age is number of consecutive unmatched detections before a track is deleted. Both parameters need to be tuned to improve the tracking and detection performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hBA8lLtsFTqN",
        "outputId": "73a337f7-8446-4fee-c390-b4b19224575b"
      },
      "outputs": [],
      "source": [
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "from collections import deque\n",
        "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "\n",
        "\n",
        "frame_count = 0               # frame counter\n",
        "\n",
        "max_age = 4                   # number of consecutive unmatched detection before the track is deleted\n",
        "\n",
        "min_hits =1                   # number of consecutive matches needed to establish a track\n",
        "\n",
        "tracker_list =[]              # list for trackers\n",
        "\n",
        "track_id_list= deque(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'])              # list for track ID\n",
        "\n",
        "debug = True\n",
        "\n",
        "def assign_detections_to_trackers(trackers, detections, iou_thrd = 0.3):\n",
        "    IOU_mat= np.zeros((len(trackers),len(detections)),dtype=np.float32)\n",
        "    for t,trk in enumerate(trackers):\n",
        "        #trk = convert_to_cv2bbox(trk) \n",
        "        for d,det in enumerate(detections):\n",
        "         #   det = convert_to_cv2bbox(det)\n",
        "            IOU_mat[t,d] = box_iou2(trk,det) \n",
        "           \n",
        "    # Here, we are using the Hungarian algorithm to solve the maximizing the sum of IOU assignment problem.\n",
        "    \n",
        "    matched_idx = linear_assignment(-IOU_mat)\n",
        "    matched_idx = np.asarray(matched_idx)\n",
        "    matched_idx = np.transpose(matched_idx)        \n",
        "\n",
        "    unmatched_trackers, unmatched_detections = [], []\n",
        "    for t,trk in enumerate(trackers):\n",
        "        if(t not in matched_idx[:,0]):\n",
        "            unmatched_trackers.append(t)\n",
        "\n",
        "    for d, det in enumerate(detections):\n",
        "        if(d not in matched_idx[:,1]):\n",
        "            unmatched_detections.append(d)\n",
        "\n",
        "    matches = []\n",
        "   \n",
        "    # To create trackers we consider any detection with an overlap less than iou_thrd to signifiy the existence of an untracked object\n",
        "    \n",
        "    for m in matched_idx:\n",
        "        if(IOU_mat[m[0],m[1]]<iou_thrd):\n",
        "            unmatched_trackers.append(m[0])\n",
        "            unmatched_detections.append(m[1])\n",
        "        else:\n",
        "            matches.append(m.reshape(1,2))\n",
        "    \n",
        "    if(len(matches)==0):\n",
        "        matches = np.empty((0,2),dtype=int)\n",
        "    else:\n",
        "        matches = np.concatenate(matches,axis=0)\n",
        "    \n",
        "    return matches, np.array(unmatched_detections), np.array(unmatched_trackers)       \n",
        "    \n",
        "\n",
        "\n",
        "def pipeline(img):\n",
        "    global frame_count\n",
        "    global tracker_list\n",
        "    global max_age\n",
        "    global min_hits\n",
        "    global track_id_list\n",
        "    global debug\n",
        "    \n",
        "    frame_count+=1\n",
        "    \n",
        "    img_dim = (img.shape[1], img.shape[0])\n",
        "    z_box = det.get_localization(img) # measurement\n",
        "    if debug:\n",
        "       print('Frame:', frame_count)\n",
        "       \n",
        "    x_box =[]\n",
        "    if debug: \n",
        "        for i in range(len(z_box)):\n",
        "           img1= draw_box_label(img.copy(), z_box[i], box_color=(255, 0, 0))\n",
        "           plt.imshow(img1)\n",
        "        plt.show()\n",
        "    \n",
        "    if len(tracker_list) > 0:\n",
        "        for trk in tracker_list:\n",
        "            x_box.append(trk.box)\n",
        "    \n",
        "    \n",
        "    matched, unmatched_dets, unmatched_trks \\\n",
        "    = assign_detections_to_trackers(x_box, z_box, iou_thrd = 0.3)  \n",
        "    if debug:\n",
        "         print('Detection: ', z_box)\n",
        "         print('x_box: ', x_box)\n",
        "         print('matched:', matched)\n",
        "         print('unmatched_det:', unmatched_dets)\n",
        "         print('unmatched_trks:', unmatched_trks)\n",
        "    \n",
        "         \n",
        "    # Working on matched detections  \n",
        "    # When the car is detected again in the second video frame, running the following assign_detections_to_trackers returns an one-element list , an empty list, \n",
        "    # for matched, unmatched_dets, and unmatched_trks, respectively.\n",
        "\n",
        "    if matched.size >0:\n",
        "        for trk_idx, det_idx in matched:\n",
        "            z = z_box[det_idx]\n",
        "            z = np.expand_dims(z, axis=0).T\n",
        "            tmp_trk= tracker_list[trk_idx]\n",
        "            tmp_trk.kalman_filter(z)\n",
        "            xx = tmp_trk.x_state.T[0].tolist()\n",
        "            xx =[xx[0], xx[2], xx[4], xx[6]]\n",
        "            x_box[trk_idx] = xx\n",
        "            tmp_trk.box =xx\n",
        "            tmp_trk.hits += 1\n",
        "            tmp_trk.no_losses = 0\n",
        "\n",
        "    # The above code block carries out two important tasks, \n",
        "    # 1) carrying out the Kalman filter's prediction and update stages using the function tmp_trk.kalman_filter(); \n",
        "    # 2) increasing the hits of the track by one tmp_trk.hits +=1. \n",
        "    \n",
        "    # In our analysis, trk.hits >= min_hits and trk.no_losses <=max_age.  So the track is fully established. As the result, the bounding box is annotated in the output image\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    # Working on  unmatched detections \n",
        "    # if len(unmatched_dets)>0:\n",
        "    #     for idx in unmatched_dets:\n",
        "    #         z = z_box[idx]\n",
        "    #         z = np.expand_dims(z, axis=0).T\n",
        "    #         tmp_trk = Tracker() # Creating a new tracker\n",
        "    #         x = np.array([[z[0], 0, z[1], 0, z[2], 0, z[3], 0]]).T\n",
        "    #         tmp_trk.x_state = x\n",
        "    #         # x = np.array([[z[0], 0, z[1], 0, z[2], 0, z[3], 0]]).T\n",
        "    #         # tmp_trk.x_state = np.reshape(tmp_trk.x_state, x.shape)\n",
        "    #         # tmp_trk.x_state = x\n",
        "    #         tmp_trk.predict_only()\n",
        "    #         xx = tmp_trk.x_state\n",
        "    #         xx = xx.T[0].tolist()\n",
        "    #         xx =[xx[0], xx[2], xx[4], xx[6]]\n",
        "    #         tmp_trk.box = xx\n",
        "    #         tmp_trk.id = track_id_list.popleft() \n",
        "    #         tracker_list.append(tmp_trk)\n",
        "    #         x_box.append(xx)\n",
        "    if len(unmatched_dets) > 0:\n",
        "        for idx in unmatched_dets:\n",
        "            z = z_box[idx]\n",
        "            z = np.array([z[0], 0, z[1], 0, z[2], 0, z[3], 0]).reshape((8, 1))\n",
        "            tmp_trk = Tracker()  # Creating a new tracker\n",
        "            tmp_trk.x_state = z\n",
        "            tmp_trk.predict_only()\n",
        "            xx = tmp_trk.x_state\n",
        "            xx = xx.T[0].tolist()\n",
        "            xx = [xx[0], xx[2], xx[4], xx[6]]\n",
        "            tmp_trk.box = xx\n",
        "            tmp_trk.id = track_id_list.popleft()\n",
        "            tracker_list.append(tmp_trk)\n",
        "            x_box.append(xx)\n",
        "\n",
        "    \n",
        "    # Working with unmatched tracks       \n",
        "    if len(unmatched_trks)>0:\n",
        "        for trk_idx in unmatched_trks:\n",
        "            tmp_trk = tracker_list[trk_idx]\n",
        "            tmp_trk.no_losses += 1\n",
        "            tmp_trk.predict_only()\n",
        "            xx = tmp_trk.x_state\n",
        "            xx = xx.T[0].tolist()\n",
        "            xx =[xx[0], xx[2], xx[4], xx[6]]\n",
        "            tmp_trk.box =xx\n",
        "            x_box[trk_idx] = xx\n",
        "                   \n",
        "       \n",
        "    # The list of tracks that are to be annotated: \n",
        "    good_tracker_list =[]\n",
        "    for trk in tracker_list:\n",
        "        if ((trk.hits >= min_hits) and (trk.no_losses <=max_age)):\n",
        "             good_tracker_list.append(trk)\n",
        "             x_cv2 = trk.box\n",
        "             if debug:\n",
        "                 print('updated box: ', x_cv2)\n",
        "                 print()\n",
        "             img= draw_box_label(img.copy(), x_cv2)  # Drawing the bounding boxes\n",
        "\n",
        "    deleted_tracks = filter(lambda x: x.no_losses >max_age, tracker_list)  \n",
        "    \n",
        "    for trk in deleted_tracks:\n",
        "            track_id_list.append(trk.id)\n",
        "    \n",
        "    tracker_list = [x for x in tracker_list if x.no_losses<=max_age]\n",
        "    \n",
        "    if debug:\n",
        "       print('Ending tracker_list: ',len(tracker_list))\n",
        "       print('Ending good tracker_list: ',len(good_tracker_list))\n",
        "    \n",
        "       \n",
        "    return img\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  det = CarDetector()\n",
        "  start=time.time()\n",
        "  output = 'test_v7.mp4'\n",
        "  clip1 = VideoFileClip(\"project_video.mp4\")     \n",
        "  clip = clip1.fl_image(pipeline)\n",
        "  clip.write_videofile(output, audio=False)\n",
        "  end  = time.time()\n",
        "    \n",
        "#     print(round(end-start, 2), 'Seconds to finish')\n",
        "#   if debug: # testing on images\n",
        "#         images = [plt.imread(file) for file in glob.glob('E:\\\\maths project\\\\VehicleVision\\\\test_images\\\\*.jpg')]\n",
        "        \n",
        "#         for i in range(len(images))[0:7]:\n",
        "#              image = images[i]\n",
        "#              image_box = pipeline(image)   \n",
        "#              plt.imshow(image_box)\n",
        "#              plt.show()\n",
        "#   else:                         # test on a video file.\n",
        "#     start=time.time()\n",
        "#     output = 'test_v7.mp4'\n",
        "#     clip1 = VideoFileClip(\"project_video.mp4\")     \n",
        "#     clip = clip1.fl_image(pipeline)\n",
        "#     clip.write_videofile(output, audio=False)\n",
        "#     end  = time.time()\n",
        "    \n",
        "#     print(round(end-start, 2), 'Seconds to finish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VioySF7c6WtC"
      },
      "source": [
        "From all the above output images, the condition trk.hits >= the number of consecutive matches need for track establishment and trk.no_losses <= the number of cosecutive unmatched detections before a track is deleted. Hence the track is fully established. \n",
        "\n",
        "As the result, the bounding box is created in the output image, as shown in the above figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm4lSX_XrEPy"
      },
      "source": [
        "So, we have implemented the detection and tracking logic which includes detection track assignment and track management.\n",
        "In this analysis, the detecter first localized the vehicles in each frame. Then the tracker is updated with detected results. Hence we predicted and updated the location based on the current state.\n",
        "Or it can also be said as we have predicted the cars next position depending upon its previous position. We have also dealt with noise measurements by looking at how the boxes ovelap on eachother depending on the noise levels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nTgJ8MtvO7w"
      },
      "source": [
        "The model has performed pretty well and produced desirable results in prediction, measurement and updation, depending the assumptions taken into consideration."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Vehicle detection and tracking project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
